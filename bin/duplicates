#!/usr/bin/env python3

"""Scans directory trees for duplicate files and does things with them.

Usage:

  duplicates -h
  duplicates [-amp] [-d|-s|-l] pathname [...]

Options:

  -a  check all pathnames, even those whose basenames begin with "."
  -d  delete duplicates, leaving only the first pathname
  -h  print this help message and exit
  -l  delete duplicates[1:] and hard-link those pathnames to duplicates[0]
  -m  the minimum file size to bother checking (default: 512)
  -p  print duplicates
  -s  delete duplicates[1:] and symlink those pathnames to duplicates[0]

`-s` and `-l` cannot both be used together, and neither can be used together
with `-d`.

TODO: Add a path-filter regular expression option.

TODO: When two pathnames are already the same underlying file (i.e.
hardlinks), do not treat them as duplicates."""

from os import stat, walk, environ as environment, link, symlink, unlink
from os.path import join as path_join, realpath
from getopt import getopt as get_options
from hashlib import sha256 as hash
from sys import exit, stderr, stdout, argv as arguments

OutputFieldSeparator = environment.get("OFS", "\n")
OutputRecordSeparator = environment.get("ORS", "\n\n")
MinimumFileSize = 512


def hash_file(pathname):
    f = open(pathname, "rb")
    h = hash()
    while True:
        d = f.read(4096)
        if not d:
            break
        h.update(d)
    return h.digest()


def print_error(error, output=stderr):
    output.write(str(error))
    output.write("\n")


def scan_tree(root, state, all=False):
    for parent, directories, files in walk(root):
        for f in files:
            try:
                f = realpath(path_join(parent, f))
                if not all and "/." in f:
                    continue
                status = stat(f)
                size = status.st_size
                if size < MinimumFileSize:
                    continue
                state.setdefault(size, set()).add(f)
            except OSError as e:
                print_error(e)


def print_duplicates(duplicates, output=stdout):
    output.write(OutputFieldSeparator.join(duplicates))
    output.write(OutputRecordSeparator)


def delete_duplicates(duplicates):
    for pathname in duplicates[1:]:
        try:
            unlink(pathname)
        except OSError as e:
            print_error(e)
            continue


def link_duplicates(duplicates, link=link):
    sourcePathname = duplicates[0]
    for pathname in duplicates[1:]:
        try:
            unlink(pathname)
            link(sourcePathname, pathname)
        except OSError as e:
            print_error(e)


def symlink_duplicates(duplicates):
    link_duplicates(duplicates, link=symlink)


def reduce_to_duplicates(state):
    for size in state:
        pathnames = state[size]
        duplicates = {}
        for p in pathnames:
            try:
                h = hash_file(p)
                duplicates.setdefault(h, []).append(p)
            except IOError as e:
                print_error(e)

        for d in duplicates:
            if len(duplicates[d]) > 1:
                yield duplicates[d]


if __name__ == "__main__":
    options, pathnames = get_options(arguments[1:], "adhlm:ps")
    options = dict(options)

    if (
        ("-l" in options and "-s" in options) or
        ("-l" in options and "-d" in options) or
        ("-s" in options and "-d" in options) or
        "-h" in options
    ):
        print(__doc__)
        exit(1)

    actions = []
    if "-d" in options:
        actions.append(delete_duplicates)
    if "-l" in options:
        actions.append(link_duplicates)
    if "-m" in options:
        MinimumFileSize = int(options["-m"])
    if "-p" in options:
        actions.append(print_duplicates)
    if "-s" in options:
        actions.append(symlink_duplicates)

    if len(actions) == 0:
        actions.append(print_duplicates)

    if len(pathnames) == 0:
        pathnames.append(".")

    potential_duplicates = {}
    for pathname in pathnames:
        scan_tree(pathname, potential_duplicates, all="-a" in options)

    for d in reduce_to_duplicates(potential_duplicates):
        for a in actions:
            a(d)
